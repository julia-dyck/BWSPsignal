% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eval.rank_auc_b.R
\name{eval.rank_auc}
\alias{eval.rank_auc}
\alias{eval.rank_auc_b}
\alias{eval.rank_auc_f}
\title{Rank WSP test configurations by AUC obtained from simulation}
\usage{
eval.rank_auc_b(perf_b)

eval.rank_auc_f(perf_f)
}
\arguments{
\item{perf_b}{data frame containing performance results for BWSP tests
returned by \link{eval.calc_perf_b}}

\item{perf_f}{data frame containing performance results for FWSP tests
returned by \link{eval.calc_perf_f}}
}
\value{
A list containing
\itemize{
\item \code{$ranking}: Ranking of fit and WSP test specifications according
to AUC averaged over all sample scenarios (- for BWSP given a correct
specification of prior belief)
\item \code{$effect.of.N}: Effect of sample size on AUC for the optimal fit
and WSP test (- for BWSP given a correct specification of prior belief)
\item \code{$effect.of.br}: Effect of background rate on AUC for the optimal fit
and WSP test (- for BWSP given a correct specification of prior belief)
\item \code{$effect.of.adr.rate}: Effect of ADR rate on AUC for the optimal fit
and WSP test (- for BWSP given a correct specification of prior belief)
\item \code{$effect.of.adr.when}: Effect of true expected event times on AUC
for the optimal fit and WSP test (- for BWSP given a correct
specification of prior belief)
\item \code{$effect.of.adr.relsd}: Effect of relative standard deviation of event
time on AUC for the optimal fit and WSP test (- for BWSP given a correct
specification of prior belief)
\item \code{$effect.of.dist.prior.to.truth}: Effect of distance of prior belief
to true adr.when on AUC for the optimal fit and WSP test
}
}
\description{
Ranks all fit (defined by time-to-event and prior distribution)
and test specifications grouped by simulation scenarios in terms
of the corresponding area under the curve (AUC) value for either Bayesian or
frequentist Weibull Shape Parameter (BWSP, FWSP) tests.
}
\details{
For definitions of the performance metrics returned in output,
see the details section of \link{eval.calc_perf}.
}
\examples{
\dontrun{
# ranking for all Bayesian WSP tests 
# input perf_b loaded or generated with eval.calc_perf_b
ranking_b = eval.rank_auc_b(perf_b)
# effects of DGP parameters on AUC of top test
ranking_b

# ranking for all frequentist WSP tests
# perf_f loaded or generated with eval.calc_perf_f
ranking_f = eval.rank_auc_f(perf_f)
# effects of DGP parameters on AUC of top test
ranking_f

# merging AUC ranking of Bayesian and frequentist tests
ranking = dplyr::bind_rows(ranking_b$ranking[, 1:8], 
                           ranking_f$ranking[, 1:5]) \%>\%
          filter(tte.dist == "pgw") \%>\%
          dplyr::arrange(dplyr::desc(AUC))
      
    
# ranking among subset of scenarios, e.g. excluding model choices that lead to
# long execution times
auc_pgw_ll <- dplyr::filter(perf_b, tte.dist == "pgw", prior.dist == "ll")
eval.rank_auc_b(auc_dw_ll) 
                               
}


}
