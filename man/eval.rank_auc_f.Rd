% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eval.rank_auc_f.R
\name{eval.rank_auc_f}
\alias{eval.rank_auc_f}
\title{Evaluation: rank FWSP tests by AUC}
\usage{
eval.rank_auc_f(perf_f)
}
\arguments{
\item{perf_f}{A data frame containing Bayesian performance metric results returned by \link{eval.calc_perf_f}.}
}
\value{
A list containing
\itemize{
\item \code{$ranking}: Ranking of fit and FWSP test specifications according
to AUC averaged over all sample scenarios.
\item \code{$effect.of.N}: Effect of sample size on AUC for the optimal fit
and FWSP test specification.
\item \code{$effect.of.br}: Effect of background rate on AUC for the optimal fit
and FWSP test specification.
\item \code{$effect.of.adr.rate}: Effect of ADR rate on AUC for the optimal fit
and FWSP test specification.
\item \code{$effect.of.adr.when}: Effect of true expected event times on AUC
for the optimal fit and FWSP test specification.
\item \code{$effect.of.adr.relsd}: Effect of relative standard deviation of event
time on AUC for the optimal fit and FWSP test specification.
}
}
\description{
Ranks all tried fit & test specifications by AUC.
}
